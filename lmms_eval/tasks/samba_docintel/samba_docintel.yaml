dataset_path: csv
dataset_kwargs:
  data_files: "/import/ml-sc-scratch1/matte/samba_docintel/samba_docintel.csv"
task: "samba_docintel"
test_split: train
output_type: generate_until
doc_to_visual: !function utils.samba_docintel_doc_to_visual
doc_to_text: !function utils.samba_docintel_doc_to_text
doc_to_target: "answer"
generation_kwargs:
  max_new_tokens: 512
  temperature: 0
  do_sample: False
process_results: !function utils.samba_docintel_process_results
metric_list:
  - metric: overall
    aggregation: mean
    higher_is_better: true
  - metric: evidence_figure
    aggregation: mean
    higher_is_better: true
  - metric: evidence_chart
    aggregation: mean
    higher_is_better: true
  - metric: evidence_pure-text_(plain-text)
    aggregation: mean
    higher_is_better: true
  - metric: evidence_table
    aggregation: mean
    higher_is_better: true
  - metric: evidence_unanswerable
    aggregation: mean
    higher_is_better: true
  - metric: format_int
    aggregation: mean
    higher_is_better: true
  - metric: format_str
    aggregation: mean
    higher_is_better: true
  - metric: format_float
    aggregation: mean
    higher_is_better: true
  - metric: format_list
    aggregation: mean
    higher_is_better: true
  - metric: format_none
    aggregation: mean
    higher_is_better: true
  - metric: skipped_questions
    aggregation: sum
    higher_is_better: False
filter_list:
  - name: "samba-extract"
    filter:
      - function: "samba_filter"
metadata:
  - version: 0.0
lmms_eval_specific_kwargs:
  default:
    pre_prompt: "Read the document and answer this question: "
    post_prompt: "\nPlease make your answer as concise as possible. If the question isn't answerable, just say 'Not answerable'."